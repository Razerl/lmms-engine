# LLaVA-Video Training Configuration
# This configuration demonstrates how to train LLaVA-Video with time instruction injection

trainer_type: fsdp2_trainer

dataset_config:
  dataset_type: llava_video
  dataset_format: yaml
  dataset_path: null
  processor_config:
    processor_name: "llava-hf/llava-onevision-qwen2-7b-ov-hf"
    processor_type: llava_video
  # Using inline datasets configuration to load parquet files
  datasets:
    - path: "/blob/llava_video_parquet/*.parquet"
      data_folder: "/blob/LLaVA-Video-178K"
      data_type: parquet
  shuffle: true
  eval_dataset_path: null
  object_storage: none
  bucket_name: null
  packing: false
  packing_strategy: first_fit
  packing_length: 8192
  filter_overlong: true
  filter_overlong_workers: 8
  max_length: null

  # Video sampling configuration
  video_sampling_strategy: fps
  frame_num: 16  # Reduced from 64 for memory efficiency
  fps: 1
  video_backend: decord

  # LLaVA-Video specific parameters
  extra_kwargs:
    data_folder: "/blob/LLaVA-Video-178K"  # Base folder for video files
    frames_upbound: 16  # Reduced from 64 to fit in memory (0.9B model)
    force_sample: true  # Force uniform sampling to frames_upbound
    add_time_instruction: true  # Inject time instruction into prompts
    # Slow-fast parameters for token calculation (must match model_config)
    faster_token_stride: 10
    mm_spatial_pool_stride: 2
    mm_spatial_pool_mode: bilinear

trainer_args:
  output_dir: ./output/llava_video_training
  overwrite_output_dir: false
  do_train: true
  do_eval: false
  do_predict: false
  eval_strategy: 'no'
  prediction_loss_only: false
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps:  2
  eval_accumulation_steps: null
  eval_delay: 0
  torch_empty_cache_steps: null

  # Optimizer configuration
  learning_rate: 1.0e-05
  weight_decay: 0.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  max_grad_norm: 1.0

  # Training schedule
  num_train_epochs: 1
  max_steps: -1
  lr_scheduler_type: cosine
  lr_scheduler_kwargs: {}
  warmup_ratio: 0.03
  warmup_steps: 0

  # Logging
  log_level: passive
  log_level_replica: warning
  log_on_each_node: true
  logging_dir: ./output/llava_video_training/runs
  logging_strategy: steps
  logging_first_step: false
  logging_steps: 1
  logging_nan_inf_filter: true

  # Checkpointing
  save_strategy: steps
  save_steps: 500
  save_total_limit: 1
  save_safetensors: true
  save_on_each_node: false
  save_only_model: false
  restore_callback_states_from_checkpoint: false

  # Device configuration
  no_cuda: false
  use_cpu: false
  use_mps_device: false
  seed: 42
  data_seed: null
  jit_mode_eval: false

  # Precision
  bf16: true
  fp16: false
  fp16_opt_level: O1
  half_precision_backend: auto
  bf16_full_eval: false
  fp16_full_eval: false
  tf32: true

  # Distributed training
  local_rank: 0
  ddp_backend: null
  tpu_num_cores: null
  tpu_metrics_debug: false
  debug: []

  # Dataloader
  dataloader_drop_last: true
  eval_steps: null
  dataloader_num_workers: 2
  dataloader_prefetch_factor: null
  past_index: -1
  run_name: llava_video_training
  disable_tqdm: false
  remove_unused_columns: false
  label_names: null
  load_best_model_at_end: false
  metric_for_best_model: null
  greater_is_better: null
  ignore_data_skip: false

  # FSDP configuration
  fsdp: []
  fsdp_min_num_params: 0
  fsdp_config:
    transformer_layer_cls_to_wrap:
    - Qwen2DecoderLayer
    reshard_after_forward: false
    min_num_params: 0
    xla: false
    xla_fsdp_v2: false
    xla_fsdp_grad_ckpt: false
  fsdp_transformer_layer_cls_to_wrap: null

  accelerator_config:
    split_batches: false
    dispatch_batches: null
    even_batches: true
    use_seedable_sampler: true
    non_blocking: false
    gradient_accumulation_kwargs: null
  parallelism_config: null
  deepspeed: null

  # Optimization
  label_smoothing_factor: 0.0
  optim: adamw_torch
  optim_args: null
  adafactor: false
  group_by_length: false
  length_column_name: length

  # Reporting
  report_to:
  - tensorboard
  - wandb
  project: huggingface
  trackio_space_id: trackio

  # Advanced
  ddp_find_unused_parameters: null
  ddp_bucket_cap_mb: null
  ddp_broadcast_buffers: null
  dataloader_pin_memory: true
  dataloader_persistent_workers: false
  skip_memory_metrics: true
  use_legacy_prediction_loop: false
  push_to_hub: false
  resume_from_checkpoint: null
  hub_model_id: null
  hub_strategy: every_save
  hub_token: <HUB_TOKEN>
  hub_private_repo: null
  hub_always_push: false
  hub_revision: null

  # Memory optimization
  gradient_checkpointing: true
  gradient_checkpointing_kwargs: null
  include_inputs_for_metrics: false
  include_for_metrics: []
  eval_do_concat_batches: true
  fp16_backend: auto
  push_to_hub_model_id: null
  push_to_hub_organization: null
  mp_parameters: ''
  auto_find_batch_size: false
  full_determinism: false
  torchdynamo: null
  ray_scope: last
  ddp_timeout: 1800

  # Compilation (optional, for speedup)
  torch_compile: true
  torch_compile_backend: inductor
  torch_compile_mode: null

  include_tokens_per_second: false
  include_num_input_tokens_seen: 'no'
  neftune_noise_alpha: null
  optim_target_modules: null
  batch_eval_metrics: false
  eval_on_start: false

  # Liger kernel for memory efficiency
  use_liger_kernel: true
  liger_kernel_config: null
  eval_use_gather_object: false
  average_tokens_across_devices: true
  use_muon: false
  freeze_modules: null

  # Remove padding for efficiency
  use_rmpad: true

  # FSDP2 configuration
  fsdp2: true
  sp_ulysses_degree: 1
  reduce_dtype: bfloat16
  output_dtype: bfloat16
  print_batch_input_steps: 5
  enable_profiler: false
  profiler_config:
    start_step: 1
    end_step: 3

model_config:
  extra_kwargs: {}
  # Start from LLaVA-OneVision checkpoint
  load_from_pretrained_path: "llava-hf/llava-onevision-qwen2-0.5b-ov-hf"
  load_from_config: null
  attn_implementation: flash_attention_2
  model_type: llava_onevision  # Use llava_onevision (was llava_video before refactoring)
  torch_dtype: bfloat16

  # Slow-fast frame configuration via monkey_patch_kwargs
  # Applying "video" patch automatically enables slow-fast frame processing
  monkey_patch_kwargs:
    patch_type: ["video"]
    # Video-specific parameters
    faster_token_stride: 10  # Every Nth frame is high-res slow frame
    mm_spatial_pool_stride: 2  # Base spatial pooling stride (slow=2, fast=4)
    mm_spatial_pool_mode: bilinear  # Pooling mode: bilinear, average, or max

extra_kwargs: null
